{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jG7ZEc_982io"
   },
   "source": [
    "#IMA206 PROJECT - Comprehension and exploration of a high-performance generative model: the StyleGAN - \n",
    "\n",
    "**Students**: \n",
    "\n",
    "*   Gabriela Barbosa Guedes Pereira\n",
    "\n",
    "* Lucas Pereira Fernandes\n",
    "\n",
    "* Lucas Oliveira Machado de Sousa\n",
    "\n",
    "* Marina de Menezes Lima\n",
    "\n",
    "\n",
    "**Advisors**: \n",
    "* Alasdair Newson\n",
    "* Gwilherm Lesné\n",
    "\n",
    "\n",
    "**June 2022 - Telecom Paris**\n",
    "\n",
    "\n",
    "StyleGAN2-ADA-PyTorch \n",
    "\n",
    "**Notes**\n",
    "\n",
    "This is based on Derrick Schultz's [SG2-ADA-PyTorch notebook](https://colab.research.google.com/github/dvschultz/stylegan2-ada-pytorch/blob/main/SG2_ADA_PyTorch.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vj4PG4_i9Alt"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGEXPcFJ9UTY"
   },
   "source": [
    "Let’s start by checking to see what GPU we’ve been assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7VVICTCvd4mc"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSV_HEoD9dxo"
   },
   "source": [
    "Next let’s connect our Google Drive account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IuVPuJmbigRs"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTjVmfSK9CYa"
   },
   "source": [
    "## Install repo\n",
    "\n",
    "The next cell will install the StyleGAN repository in Google Drive. If you have already installed it it will just move into that folder. If you don’t have Google Drive connected it will just install the necessary code in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B8ADVNpBh8Ox"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "!pip install gdown --upgrade\n",
    "\n",
    "if os.path.isdir(\"/content/drive/MyDrive/colab-sg2-ada-pytorch\"):\n",
    "    %cd \"/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch\"\n",
    "elif os.path.isdir(\"/content/drive/\"):\n",
    "    #install script\n",
    "    %cd \"/content/drive/MyDrive/\"\n",
    "    !mkdir colab-sg2-ada-pytorch\n",
    "    %cd colab-sg2-ada-pytorch\n",
    "    !git clone https://github.com/dvschultz/stylegan2-ada-pytorch\n",
    "    %cd stylegan2-ada-pytorch\n",
    "    !mkdir downloads\n",
    "    !mkdir datasets\n",
    "    !mkdir pretrained\n",
    "    !gdown --id 1-5xZkD8ajXw1DdopTkH_rAoCsD72LhKU -O /content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/pretrained/wikiart.pkl\n",
    "else:\n",
    "    !git clone https://github.com/dvschultz/stylegan2-ada-pytorch\n",
    "    %cd stylegan2-ada-pytorch\n",
    "    !mkdir downloads\n",
    "    !mkdir datasets\n",
    "    !mkdir pretrained\n",
    "    %cd pretrained\n",
    "    !gdown --id 1-5xZkD8ajXw1DdopTkH_rAoCsD72LhKU\n",
    "    %cd ../\n",
    "\n",
    "!pip install ninja opensimplex torch==1.7.1 torchvision==0.8.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZkcJ58P97Ls"
   },
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "Upload a .zip of square images to the `datasets` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5B-h6FpB9FaK"
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bNc-3wTO-MUd"
   },
   "source": [
    "Below are a series of variables you need to set to run the training. You probably won’t need to touch most of them.\n",
    "\n",
    "* `dataset_path`: this is the path to your .zip file\n",
    "* `resume_from`: we've uploaded the \"metfaces\" file for network parameters.\n",
    "* `mirror_x` and `mirror_y`: Allow the dataset to use horizontal or vertical mirroring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JV0W6yxP-UIn"
   },
   "outputs": [],
   "source": [
    "resume_from = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/transfer-learning-source-nets/celebahq-res256-mirror-paper256-kimg100000-ada-target0.5.pkl'\n",
    "aug_strength = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JgpdHaFn6VYz"
   },
   "outputs": [],
   "source": [
    "!wget https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/transfer-learning-source-nets/celebahq-res256-mirror-paper256-kimg100000-ada-target0.5.pkl\n",
    "!mv celebahq-res256-mirror-paper256-kimg100000-ada-target0.5.pkl /content/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "anxXXgMBPLsQ"
   },
   "outputs": [],
   "source": [
    "%cd /content/\n",
    "!git clone https://github.com/gtamba/pytorch-slim-cnn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CFvnwyd6UKoS"
   },
   "outputs": [],
   "source": [
    "%cd pytorch-slim-cnn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y346hFBfSGb7"
   },
   "outputs": [],
   "source": [
    "from slimnet import SlimNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8oIqg9FRUuBn"
   },
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gJDgXiOb7Bu2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import PIL\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o6K6eiZSU47I"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "model = SlimNet.load_pretrained('/content/pytorch-slim-cnn/models/celeba_20.pth').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oLUuWjMQbMqO"
   },
   "outputs": [],
   "source": [
    "labels = np.array(['5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes',\n",
    "       'Bald', 'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair', 'Blond_Hair',\n",
    "       'Blurry', 'Brown_Hair', 'Bushy_Eyebrows', 'Chubby', 'Double_Chin',\n",
    "       'Eyeglasses', 'Goatee', 'Gray_Hair', 'Heavy_Makeup', 'High_Cheekbones',\n",
    "       'Male', 'Mouth_Slightly_Open', 'Mustache', 'Narrow_Eyes', 'No_Beard',\n",
    "       'Oval_Face', 'Pale_Skin', 'Pointy_Nose', 'Receding_Hairline',\n",
    "       'Rosy_Cheeks', 'Sideburns', 'Smiling', 'Straight_Hair', 'Wavy_Hair',\n",
    "       'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Lipstick',\n",
    "       'Wearing_Necklace', 'Wearing_Necktie', 'Young'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2eYtGTPCZ5wt"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                              transforms.Resize((178,218)),\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LHX0A3hv6cDB"
   },
   "outputs": [],
   "source": [
    "with open('/content/celebahq-res256-mirror-paper256-kimg100000-ada-target0.5.pkl', 'rb') as f:\n",
    "    G = pickle.load(f)['G_ema'].cuda()  # torch.nn.Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7xiFWLIj85m"
   },
   "source": [
    "## Get images with/without glasses\n",
    "\n",
    "This was executed once to identify multiple images with and without the presence of glasses. Then, we handpicked the good ones (since some of the images were rather badly generated) and saved these images and the points in latent space used to generate them into .npy files, so we don't have to run the code and handpick the images again every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lhKlFR7Ea-ED"
   },
   "outputs": [],
   "source": [
    "w_glasses = []\n",
    "for cont in range(400):\n",
    "  target = set([''])\n",
    "  while 'Eyeglasses' not in target:\n",
    "    n_imgs = 1\n",
    "    z = torch.randn([n_imgs, G.z_dim]).cuda()    # latent codes\n",
    "    c = None                                # class labels (not used in this example)]\n",
    "    w = G.mapping(z, c, truncation_psi=1, truncation_cutoff=8)\n",
    "    img = G.synthesis(w, noise_mode='const', force_fp32=True)\n",
    "\n",
    "    img = (img.squeeze(0).permute(1, 2, 0) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
    "    x = transform(PIL.Image.fromarray(img.cpu().numpy(), 'RGB')).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "      model.eval()\n",
    "      logits = model(x)\n",
    "      sigmoid_logits = torch.sigmoid(logits)\n",
    "      predictions = (sigmoid_logits > 0.5).squeeze().cpu().numpy()\n",
    "    target = set(labels[predictions.astype(bool)])\n",
    "  w_glasses.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r0aT6MGN9vbV"
   },
   "outputs": [],
   "source": [
    "for i in range(400):\n",
    "  img = G.synthesis(w_glasses[i], noise_mode='const', force_fp32=True)\n",
    "  img = (img.squeeze(0).permute(1, 2, 0) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
    "  img_print = np.array(PIL .Image.fromarray(img.cpu().numpy(), 'RGB'))\n",
    "  plt.ion()\n",
    "  plt.figure()\n",
    "  plt.imshow(img_print)\n",
    "  plt.title(f'{i}')\n",
    "  plt.show()\n",
    "  _ = input('')\n",
    "  plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hlgy9KpJjnFN"
   },
   "outputs": [],
   "source": [
    "w_noglasses = []\n",
    "for cont in range(400):\n",
    "  target = set(['Eyeglasses'])\n",
    "  while 'Eyeglasses' in target:\n",
    "    n_imgs = 1\n",
    "    z = torch.randn([n_imgs, G.z_dim]).cuda()    # latent codes\n",
    "    c = None                                # class labels (not used in this example)]\n",
    "    w = G.mapping(z, c, truncation_psi=1, truncation_cutoff=8)\n",
    "    img = G.synthesis(w, noise_mode='const', force_fp32=True)\n",
    "\n",
    "    img = (img.squeeze(0).permute(1, 2, 0) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
    "    x = transform(PIL.Image.fromarray(img.cpu().numpy(), 'RGB')).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "      model.eval()\n",
    "      logits = model(x)\n",
    "      sigmoid_logits = torch.sigmoid(logits)\n",
    "      predictions = (sigmoid_logits > 0.5).squeeze().cpu().numpy()\n",
    "    target = set(labels[predictions.astype(bool)])\n",
    "  w_noglasses.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Adq3j6AG2qSv"
   },
   "outputs": [],
   "source": [
    "for i in range(400):\n",
    "  img = G.synthesis(w_noglasses[i], noise_mode='const', force_fp32=True)\n",
    "  img = (img.squeeze(0).permute(1, 2, 0) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
    "  img_print = np.array(PIL .Image.fromarray(img.cpu().numpy(), 'RGB'))\n",
    "  plt.ion()\n",
    "  plt.figure()\n",
    "  plt.imshow(img_print)\n",
    "  plt.title(f'{i}')\n",
    "  plt.show()\n",
    "  _ = input('')\n",
    "  plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBm71rvpVwwx"
   },
   "source": [
    "Below we include the indexes of the handpicked images to save them as .npy files. If you choose to generate the images again, you'll have to handpick the images and change the indexes below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TTv27nr22-o2"
   },
   "outputs": [],
   "source": [
    "glasses_index = [0,5,6,7,8,9,10,15,20,21,23,24,26,28,31,36,38,40,41,42,48,51,52,53,54,56,58,59,\n",
    "61,62,63,64,69,70,72,78,79,87,88,91,95,98,99,100,104,105,114,115,125,131,133,\n",
    "139,144,150,151,153,156,162,175]\n",
    "\n",
    "noglasses_index = [0,1,2,3,4,5,6,7,8,9,11,12,13,14,15,16,17,18,20,25,26,27,28,29,31,32,\n",
    "35,37,38,41,42,45,46,49,50,57,58,59,60,61,62,63,64,65,66,67,68,70,71,\n",
    "72,73,74,75,76,77,78,79,80,82]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_rGtqUuB63MQ"
   },
   "outputs": [],
   "source": [
    "w_glasses_list = np.array([x.cpu().numpy() for x in w_glasses])\n",
    "w_glasses_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eNDs0egx76l-"
   },
   "outputs": [],
   "source": [
    "w_glasses_selected = w_glasses_list[glasses_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YY3gj3ci8yx3"
   },
   "outputs": [],
   "source": [
    "w_noglasses_list = np.array([x.cpu().numpy() for x in w_noglasses])\n",
    "w_noglasses_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kA3fMjyh8_zj"
   },
   "outputs": [],
   "source": [
    "w_noglasses_selected = w_noglasses_list[noglasses_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qpI6V_1A-fjK"
   },
   "outputs": [],
   "source": [
    "np.save('/content/w_glasses_list.npy', np.array(w_glasses_list))\n",
    "np.save('/content/w_noglasses_list.npy', np.array(w_noglasses_list))\n",
    "np.save('/content/w_glasses_selected.npy', np.array(w_glasses_selected))\n",
    "np.save('/content/w_noglasses_selected.npy', np.array(w_noglasses_selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_dBtCfGcN7b"
   },
   "outputs": [],
   "source": [
    "logits = model(x)\n",
    "sigmoid_logits = torch.sigmoid(logits)\n",
    "predictions = (sigmoid_logits > 0.5).squeeze().numpy()\n",
    "\n",
    "print(labels[predictions.astype(bool)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "09A2l-ysGBza"
   },
   "outputs": [],
   "source": [
    "img_testing = G.synthesis(w[35].unsqueeze(0), noise_mode='const', force_fp32=True)\n",
    "img_testing = (img_testing.squeeze(0).permute(1, 2, 0) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
    "img_testing = np.array(PIL.Image.fromarray(img_testing.cpu().numpy(), 'RGB'))\n",
    "plt.figure()\n",
    "imshow(img_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7TaCZO1IkSu"
   },
   "outputs": [],
   "source": [
    "sunglasses.append(w[11])\n",
    "glasses.append(w[34])\n",
    "hat.append(w[36])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlrjc_ickr5l"
   },
   "source": [
    "## Load saved data (see above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tr0vlY0jm2XZ"
   },
   "outputs": [],
   "source": [
    "def generate_img(G,w=None,z=None):\n",
    "  c=None\n",
    "  if(z!=None):\n",
    "    w = G.mapping(z, c, truncation_psi=0.8, truncation_cutoff=8)\n",
    "  # G is a Generator and w is the point in the mapping space (as a tensor)\n",
    "  img = G.synthesis(w, noise_mode='const', force_fp32=True)\n",
    "  img = (img.squeeze(0).permute(1, 2, 0) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
    "  img = img.cpu().numpy()\n",
    "  return img\n",
    "\n",
    "def img_to_tensor(img):\n",
    "  return transform(PIL.Image.fromarray(img, 'RGB')).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkU9bLP9rHvH"
   },
   "source": [
    "## Moving in Z latent space\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GC-r6uO7XcAR"
   },
   "source": [
    "Below we download the saved Z values for the handpicked \"glasses/no glasses\" images and we use a SVM to find a separating hyperplane between these two glasses. Then, we do the same for the \"young/old\" classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y7wHJjJ-2mt1"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TeJKFzLAX3OR"
   },
   "source": [
    "### Glasses/no glasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B73gv4ZYuOP4"
   },
   "outputs": [],
   "source": [
    "!gdown --id 1VQl5ICbllJs1ePTGwndM3Y2QVNMMqaD0 -O /content/z_no_glasses_selected.npy\n",
    "!gdown --id 1PQXKCVK7_HY5B9moVgDHhMPWsGR4lB5U -O /content/z_glasses_selected.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Byy4Zcn_vo_1"
   },
   "outputs": [],
   "source": [
    "z_noglasses_selected = torch.from_numpy(np.load(\"/content/z_no_glasses_selected.npy\"))\n",
    "z_glasses_selected = torch.from_numpy(np.load(\"/content/z_glasses_selected.npy\"))\n",
    "\n",
    "z_noglasses_selected = z_noglasses_selected.to(device)\n",
    "z_glasses_selected = z_glasses_selected.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B3Y_MOMFvsBV"
   },
   "outputs": [],
   "source": [
    "img_glasses = [generate_img(G,z=z) for z in z_glasses_selected]\n",
    "img_noglasses = [generate_img(G,z=z) for z in z_noglasses_selected]\n",
    "\n",
    "# Transform tensors to numpy\n",
    "z_noglasses_arr = np.squeeze(z_noglasses_selected.cpu().numpy())\n",
    "z_glasses_arr = np.squeeze(z_glasses_selected.cpu().numpy())\n",
    "\n",
    "# Space shape\n",
    "space_shape = z_noglasses_arr[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ndVPzVEzyK9M"
   },
   "outputs": [],
   "source": [
    "# Create X (points in mapping space) and y (classes)\n",
    "X = np.concatenate((z_noglasses_arr, z_glasses_arr))\n",
    "y = np.concatenate((np.zeros(z_noglasses_arr.shape[0]), np.ones(z_glasses_arr.shape[0])))\n",
    "\n",
    "# Sort the samples\n",
    "idx = random.sample(list(range(len(y))), len(y))\n",
    "X = X[idx].reshape(X.shape[0], -1)\n",
    "y = y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yCKR5WHovkHM"
   },
   "outputs": [],
   "source": [
    "# Fit SVM to find the separating hyperplane\n",
    "\n",
    "clf_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                          ('clf', SVC(gamma='auto', kernel='linear'))])\n",
    "clf_pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-vSfJTqysXA"
   },
   "outputs": [],
   "source": [
    "# Get hyperplane direction and normalize it\n",
    "\n",
    "hyperplane_glasses = clf_pipe['clf'].coef_[0]\n",
    "hyperplane_glasses = hyperplane_glasses.reshape(space_shape)\n",
    "hyperplane_glasses = torch.from_numpy(hyperplane_glasses).to(device)\n",
    "print(f'norm = {torch.norm(hyperplane_glasses)}')\n",
    "hyperplane_glasses /= torch.norm(hyperplane_glasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfOr5_tAZLYI"
   },
   "source": [
    "Below, we use the hyperplane direction to move an image from a point in the \"without glasses\" side of the hyperplane to a point in the \"with glasses\" side of the hyperplane, moving in latent space Z, and we save these images as a gif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Osgug5WTyyyV"
   },
   "outputs": [],
   "source": [
    "drive_path = \"/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/saved_images/\"\n",
    "import imageio\n",
    "images = []\n",
    "img_n = 13\n",
    "for i in range(30):\n",
    "  z = z_noglasses_selected[img_n] + hyperplane_glasses*i\n",
    "  img = generate_img(G, z=z)\n",
    "  images.append(img)\n",
    "imageio.mimsave(drive_path + f'moving_z_glasses_{img_n}.gif', images, duration=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IaESbBBA0Ymk"
   },
   "source": [
    "#### Old/young"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qk1ia9ii0YL9"
   },
   "outputs": [],
   "source": [
    "!gdown --id 1Dlb8pv7hd-HEAmDkLBqar1DOT4QqTk3I -O /content/z_young_selected.npy\n",
    "!gdown --id 1lzNq_YGGsjaEE2UlRCBNIM4Ygkksyr-H -O /content/z_old_selected.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xb9LS1og0LeM"
   },
   "outputs": [],
   "source": [
    "z_young_selected = torch.from_numpy(np.load(\"/content/z_young_selected.npy\"))\n",
    "z_old_selected = torch.from_numpy(np.load(\"/content/z_old_selected.npy\"))\n",
    "\n",
    "z_young_selected = z_young_selected.to(device)\n",
    "z_old_selected = z_old_selected.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MiwM0fnZ1Igo"
   },
   "outputs": [],
   "source": [
    "img_young = [generate_img(G,z=z) for z in z_young_selected]\n",
    "img_old = [generate_img(G,z=z) for z in z_old_selected]\n",
    "\n",
    "# Transform tensors to numpy\n",
    "z_young_arr = np.squeeze(z_young_selected.cpu().numpy())\n",
    "z_old_arr = np.squeeze(z_old_selected.cpu().numpy())\n",
    "\n",
    "# Space shape\n",
    "space_shape = z_young_arr[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dz2LBcHI1cZ1"
   },
   "outputs": [],
   "source": [
    "# Create X (points in mapping space) and y (classes)\n",
    "X = np.concatenate((z_young_arr, z_old_arr))\n",
    "y = np.concatenate((np.zeros(z_young_arr.shape[0]), np.ones(z_old_arr.shape[0])))\n",
    "\n",
    "# Sort the samples\n",
    "idx = random.sample(list(range(len(y))), len(y))\n",
    "X = X[idx].reshape(X.shape[0], -1)\n",
    "y = y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8CNz8Mzq1rsB"
   },
   "outputs": [],
   "source": [
    "# Fit SVM to find the separating hyperplane\n",
    "\n",
    "clf_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                          ('clf', SVC(gamma='auto', kernel='linear'))])\n",
    "clf_pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F9ffY9By15Em"
   },
   "outputs": [],
   "source": [
    "# Get hyperplane direction and normalize it\n",
    "\n",
    "hyperplane_age = clf_pipe['clf'].coef_[0]\n",
    "hyperplane_age = hyperplane_age.reshape(space_shape)\n",
    "hyperplane_age = torch.from_numpy(hyperplane_age).to(device)\n",
    "print(f'norm = {torch.norm(hyperplane_age)}')\n",
    "hyperplane_age /= torch.norm(hyperplane_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hsak3ZegYJxv"
   },
   "source": [
    "Below, we use the hyperplane direction to move an image from a point in the \"young\" side of the hyperplane to a point in the \"old\" side of the hyperplane, moving in latent space Z, and we save these images as a gif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nx1CgzQi2EX_"
   },
   "outputs": [],
   "source": [
    "drive_path = \"/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/saved_images/\"\n",
    "import imageio\n",
    "images = []\n",
    "img_n = 13\n",
    "for i in range(30):\n",
    "  z = z_young_selected[img_n] + hyperplane_age*i\n",
    "  img = generate_img(G, z=z)\n",
    "  images.append(img)\n",
    "imageio.mimsave(drive_path + f'moving_z_age_{img_n}.gif', images, duration=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOUNZt542qo3"
   },
   "source": [
    "### Conditioning in Z space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2RaMZg7ZddF"
   },
   "source": [
    "Now we'll use the method proposed in the article as \"Conditional manipulation via subspace projection\". After finding the hyperplanes corresponding to two different features, we project one plane into the other and subtract this projection from the first plane to find a direction perpendicular to the second one. In that way, when we move in this direction we're not supposed to change the other feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nuihZ_bp20U2"
   },
   "outputs": [],
   "source": [
    "hyperplane_glasses_conditioned = hyperplane_glasses-torch.matmul(hyperplane_glasses.T,hyperplane_age)*hyperplane_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nm3fxZQ_9dbT"
   },
   "outputs": [],
   "source": [
    "torch.matmul(hyperplane_glasses_conditioned,hyperplane_glasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oNKmbZUU9Kj7"
   },
   "outputs": [],
   "source": [
    "torch.matmul(hyperplane_glasses_conditioned,hyperplane_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J_maeKUm4rad"
   },
   "outputs": [],
   "source": [
    "drive_path = \"/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/saved_images/\"\n",
    "import imageio\n",
    "images = []\n",
    "img_n = 13\n",
    "for i in range(30):\n",
    "  z = z_noglasses_selected[img_n] + hyperplane_glasses_conditioned*i\n",
    "  img = generate_img(G, z=z)\n",
    "  images.append(img)\n",
    "imageio.mimsave(drive_path + f'moving_z_conditioned_{img_n}.gif', images, duration=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTowtPucP3TL"
   },
   "source": [
    "## Using hundreds of images for getting the SVM:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHYvmLjmb-cb"
   },
   "source": [
    "As an alternative to handpicking images which fit our features criteria, we've tried using the SlimNet classifier to classify a thousand of images from each class studied: with glasses, without glasses, young, old, female and male. We found separating hyperplanes as before and saved the gifs showing the result from the transition between two points in latent space. The results with SlimNet classified samples was not that good, especially concerning the \"glasses\" case of study. Checking manually, we found that the accuracy of the \"Eyeglasses\" label on this dataset was lesser than 50%, i.e., many \"no glasses\" images were labeled as \"glasses\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oTr1uUGbaNN"
   },
   "source": [
    "### Glasses / No glasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXlujkp2QRqy"
   },
   "outputs": [],
   "source": [
    "# Generating images with/without glasses\n",
    "\n",
    "w_glasses = []\n",
    "w_no_glasses = []\n",
    "i_g = 0\n",
    "i_ng = 0\n",
    "n_imgs = 1000\n",
    "idx = list(labels).index(\"Eyeglasses\")\n",
    "\n",
    "while i_g < n_imgs or i_ng < n_imgs:\n",
    "  z = torch.randn([1, G.z_dim]).cuda()    # latent codes\n",
    "  c = None                                # class labels (not used in this example)]\n",
    "  w = G.mapping(z, c, truncation_psi=1, truncation_cutoff=8)\n",
    "  img = G.synthesis(w, noise_mode='const', force_fp32=True)\n",
    "\n",
    "  img = (img.squeeze(0).permute(1, 2, 0) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
    "  x = transform(PIL.Image.fromarray(img.cpu().numpy(), 'RGB')).unsqueeze(0).to(device)\n",
    "  with torch.no_grad():\n",
    "    model.eval()\n",
    "    logits = model(x)\n",
    "    sigmoid_logits = torch.sigmoid(logits)\n",
    "    predictions = (sigmoid_logits > 0.5).squeeze().cpu().numpy()\n",
    "  \n",
    "  x_labels = set(labels[predictions.astype(bool)])\n",
    "  if \"Eyeglasses\" in x_labels:\n",
    "    if i_g < n_imgs:\n",
    "      w_glasses.append(w)\n",
    "      i_g += 1\n",
    "  else:\n",
    "    if i_ng < n_imgs:\n",
    "      w_no_glasses.append(w)\n",
    "      i_ng += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IwP9dMDpx8JG"
   },
   "outputs": [],
   "source": [
    "# Transform tensors to numpy\n",
    "w_no_glasses_arr = [w.cpu().numpy().flatten() for w in w_no_glasses]\n",
    "w_glasses_arr = [w.cpu().numpy().flatten() for w in w_glasses]\n",
    "\n",
    "# Space shape\n",
    "space_shape = w_glasses_arr[0].shape\n",
    "\n",
    "# Create X (points in mapping space) and y (classes)\n",
    "X = np.concatenate((w_no_glasses_arr, w_glasses_arr))\n",
    "y = np.concatenate((np.zeros(n_imgs), np.ones(n_imgs)))\n",
    "\n",
    "# Sort the samples\n",
    "idx = random.sample(list(range(len(y))), len(y))\n",
    "X = X[idx].reshape(X.shape[0], -1)\n",
    "y = y[idx]\n",
    "\n",
    "# Getting SVM hyperplane:\n",
    "\n",
    "clf_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                          ('clf', SVC(gamma='auto', kernel='linear'))])\n",
    "clf_pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1y1czuDSYPPz"
   },
   "outputs": [],
   "source": [
    "# Generate GIF with one of the images:\n",
    "orig_shape = [1, 14, 512]\n",
    "\n",
    "hyperplane_glasses = clf_pipe['clf'].coef_[0]\n",
    "hyperplane_glasses = hyperplane_glasses.reshape(orig_shape)\n",
    "hyperplane_glasses = torch.from_numpy(hyperplane_glasses).to(device)\n",
    "print(f'norm = {torch.norm(hyperplane_glasses)}')\n",
    "hyperplane_glasses /= torch.norm(hyperplane_glasses)\n",
    "\n",
    "drive_path = \"/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/saved_images/\"\n",
    "import imageio\n",
    "images = []\n",
    "img_n = 151\n",
    "for i in range(100):\n",
    "  w = w_no_glasses[img_n] + hyperplane_glasses*i\n",
    "  img = generate_img(G, w)\n",
    "  images.append(img)\n",
    "imageio.mimsave(drive_path + f'SVM_1000_glasses_{img_n}.gif', images, duration=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qswbt1bDbfMG"
   },
   "source": [
    "### Old / Young"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A8DCJW03ZOLZ"
   },
   "outputs": [],
   "source": [
    "# Generating images with/without glasses\n",
    "\n",
    "w_young = []\n",
    "w_old = []\n",
    "i_g = 0\n",
    "i_ng = 0\n",
    "n_imgs = 1000\n",
    "\n",
    "while i_g < n_imgs or i_ng < n_imgs:\n",
    "  z = torch.randn([1, G.z_dim]).cuda()    # latent codes\n",
    "  c = None                                # class labels (not used in this example)]\n",
    "  w = G.mapping(z, c, truncation_psi=1, truncation_cutoff=8)\n",
    "  img = G.synthesis(w, noise_mode='const', force_fp32=True)\n",
    "\n",
    "  img = (img.squeeze(0).permute(1, 2, 0) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
    "  x = transform(PIL.Image.fromarray(img.cpu().numpy(), 'RGB')).unsqueeze(0).to(device)\n",
    "  with torch.no_grad():\n",
    "    model.eval()\n",
    "    logits = model(x)\n",
    "    sigmoid_logits = torch.sigmoid(logits)\n",
    "    predictions = (sigmoid_logits > 0.5).squeeze().cpu().numpy()\n",
    "  \n",
    "  x_labels = set(labels[predictions.astype(bool)])\n",
    "  if \"Young\" in x_labels:\n",
    "    if i_g < n_imgs:\n",
    "      w_young.append(w)\n",
    "      i_g += 1\n",
    "  else:\n",
    "    if i_ng < n_imgs:\n",
    "      w_old.append(w)\n",
    "      i_ng += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WdxuoZN_b0Z7"
   },
   "outputs": [],
   "source": [
    "# Transform tensors to numpy\n",
    "w_old_arr = [w.cpu().numpy().flatten() for w in w_old]\n",
    "w_young_arr = [w.cpu().numpy().flatten() for w in w_young]\n",
    "\n",
    "# Space shape\n",
    "space_shape = w_young_arr[0].shape\n",
    "\n",
    "# Create X (points in mapping space) and y (classes)\n",
    "X = np.concatenate((w_old_arr, w_young_arr))\n",
    "y = np.concatenate((np.zeros(n_imgs), np.ones(n_imgs)))\n",
    "\n",
    "# Sort the samples\n",
    "idx = random.sample(list(range(len(y))), len(y))\n",
    "X = X[idx].reshape(X.shape[0], -1)\n",
    "y = y[idx]\n",
    "\n",
    "# Getting SVM hyperplane:\n",
    "\n",
    "clf_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                          ('clf', SVC(gamma='auto', kernel='linear'))])\n",
    "clf_pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q9p9IWGKcBKR"
   },
   "outputs": [],
   "source": [
    "# Generate GIF with one of the images:\n",
    "orig_shape = [1, 14, 512]\n",
    "\n",
    "hyperplane_age = clf_pipe['clf'].coef_[0]\n",
    "hyperplane_age = hyperplane_age.reshape(orig_shape)\n",
    "hyperplane_age = torch.from_numpy(hyperplane_age).to(device)\n",
    "print(f'norm = {torch.norm(hyperplane_age)}')\n",
    "hyperplane_age /= torch.norm(hyperplane_age)\n",
    "\n",
    "drive_path = \"/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/saved_images/\"\n",
    "import imageio\n",
    "images = []\n",
    "img_n = 577\n",
    "for i in range(150):\n",
    "  w = w_young[img_n] + hyperplane_age*i\n",
    "  img = generate_img(G, w)\n",
    "  images.append(img)\n",
    "imageio.mimsave(drive_path + f'SVM_1000_age_{img_n}.gif', images, duration=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQvJxefF2Xa8"
   },
   "source": [
    "## Quantitative evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0ccPXxcgMLc"
   },
   "source": [
    "We keep the code below for documentation purposes, but we were not able to test the evaluation metrics defined in the article. Both of the metrics proposed (path length and linear separability) need the full dataset saved as Multiresolution TFRecords. Moreover, the \"linear separability\" metric isn't available for the official PyTorch implementation of the Stylegan2 ADA, which's the source code we're using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ab8ZUed83s6D"
   },
   "outputs": [],
   "source": [
    "# Generate image for each selected point\n",
    "\n",
    "img_young = [generate_img(G,w) for w in w_young_selected]\n",
    "img_old = [generate_img(G,w) for w in w_old_selected]\n",
    "\n",
    "# Transform tensors to numpy\n",
    "w_young_arr = np.squeeze(w_young_selected.cpu().numpy())\n",
    "w_old_arr = np.squeeze(w_old_selected.cpu().numpy())\n",
    "\n",
    "# Space shape\n",
    "space_shape = w_young_arr[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oDsqCy6Y69B8"
   },
   "outputs": [],
   "source": [
    "w = w_young_selected[0]\n",
    "\n",
    "img = G.synthesis(w, noise_mode='const', force_fp32=True)\n",
    "img = (img.squeeze(0).permute(1, 2, 0) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
    "img = img.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1tnN6c5O7Z6M"
   },
   "outputs": [],
   "source": [
    "!gdown --id 1Dlb8pv7hd-HEAmDkLBqar1DOT4QqTk3I -O /content/z_young_selected.npy\n",
    "\n",
    "z_young_selected = torch.from_numpy(np.load(\"/content/z_young_selected.npy\"))\n",
    "z = z_young_selected[0].cuda()\n",
    "w = G.mapping(z, c, truncation_psi=1, truncation_cutoff=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kLMj_vSv7gIj"
   },
   "outputs": [],
   "source": [
    "drive_path = \"/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/saved_images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hzgZNgvK7gCb"
   },
   "outputs": [],
   "source": [
    "!python calc_metrics.py --metrics=ppl2_wend --network=/content/celebahq-res256-mirror-paper256-kimg100000-ada-target0.5.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VeJm2g9xL1-o"
   },
   "outputs": [],
   "source": [
    "#!git clone https://github.com/NVlabs/stylegan"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "IMA206_PROJECT_FERNANDES_GUEDES-PEREIRA_LIMA_OLIVEIRA.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
